# `static-lang-word-lists`

A collection of word lists for various scripts, compressed at build time, baked into the binary, and decompressed lazily at run time.

## Motivation

Include word lists in the binary, don't take up more space than necessary, be publishable on crates.io (10 MiB size limit)

## Usage

On crates.io as `static-lang-word-lists`

For documentation, please refer to [docs.rs](https://docs.rs/static-lang-word-lists).

## How this crate works

A build script that downloads the word lists from GitHub, compresses them with [Brotli](https://brotli.org/), and embeds that data in the binary, lazily decompressed at runtime

### Technical details

Note: adding or removing a wordlist requires that `cargo xtask slwl` be re-run.
See the [xtasks' README](../xtask/README.md) for more details on what it's doing.
This README only concerns the build script's role

1. A list of all the supported wordlists is generated by `cargo xtask slwl` (this step is manual and must be run when adding or removing a word list!)
2. The build script reads the list of paths generated by the xtask (`chicken.rs`, which is `include!`d in the build script)
3. If building from remote sources, a zipball of the repo is downloaded from GitHub, and the selected word lists are extracted (word lists may be enabled or disabled through feature flags)
4. Selected word lists are compressed with brotli (compression level is reduced in debug builds to speed up crate build time) and are written to `OUT_DIR` under their relative path, where `static-lang-word-lists/src/declarations.rs` is expecting them

## Developing

To build using local files, set the `STATIC_LANG_WORD_LISTS_LOCAL` environment variable

### Adding a new word list

1. Add the word lists as .txt files into `static-lang-word-lists/data`, in a subdirectory with a kebab-case name for your source
2. For each .txt file, create a corresponding TOML file with the same stem. See [the schema](#word-list-metadata-schema) for the required & optional fields
3. Run `cargo xtask slwl`. It'll emit crate feature definitions to stdout, copy & paste over the existing `[feature]` table in [`static-lang-word-lists/Cargo.toml`](Cargo.toml)
4. Check the crate builds with `cargo build --package static-lang-word-lists`. You will need the `STATIC_LANG_WORD_LISTS_LOCAL` environment variable set

### Word list metadata schema

Metadata files are [TOML](https://toml.io/en/v1.0.0) files.
The ones that live in this crate have the same file name as their word list, only differing in extension.

| Field name | Field type | Required? | Description                                                                           |
|:----------:|:----------:|:---------:|---------------------------------------------------------------------------------------|
|   `name`   |   string   |    ✔️     | A cosmetic name for the word list, usually in snake_case                              |
|  `script`  |   string   |     ❌     | An [ISO 15924](https://en.wikipedia.org/wiki/ISO_15924) four-letter capitalised code* |
| `language` |   string   |     ❌     | An [ISO 639-1](https://en.wikipedia.org/wiki/ISO_639-1) two-letter lowercase code*    |

(* this is not enforced, but will at least be true of crate-provided word lists.)

## Credits

Diffenator wordlists are from [diffenator2](https://github.com/googlefonts/diffenator2). [Apache-2.0](https://github.com/googlefonts/diffenator2/blob/69a873d79811e957aa5824e04d4859717f206c47/LICENSE.txt) licensed.

Emoji wordlists are from [unicode.org](https://home.unicode.org/). [Unicode](https://www.unicode.org/license.txt) licensed.

AOSP word lists are from the [aosp-test-texts](https://github.com/googlefonts/aosp-test-texts), using the files produced by `scripts/extract_words.py`. [Apache-2.0](https://github.com/googlefonts/aosp-test-texts/blob/c8134e8ae2be52feb842df5cb5fa03d29e3df06f/LICENSE) licensed.
