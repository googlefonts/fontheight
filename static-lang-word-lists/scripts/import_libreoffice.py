#!/usr/bin/env -S uv run --script
# /// script
# requires-python = ">=3.11"
# dependencies = [
#     "babelfish>=0.6",
#     "pyahocorasick>=2.2,<3",
#     "tomli-w",
# ]
# ///

"""
Generates word lists from Libreoffice dictionaries and reduces them to a more
manageable size.

Requires `hunspell-reader` to be available in $PATH, which you can get from
here:
https://github.com/streetsidesoftware/cspell/tree/main/packages/hunspell-reader#readme
(it's a Node package).
"""

import re
import subprocess
from argparse import ArgumentParser, Namespace
from pathlib import Path
from time import time
from typing import Collection, Iterable, Iterator

import ahocorasick
import tomli_w
from babelfish import Language, Script
from babelfish.exceptions import LanguageConvertError

DATA = Path(__file__).parent.parent / "data"
OUTPUT = DATA / "libreoffice"


# Dictionaries that end up being so big they need world-ending amounts of RAM
# for Node. You can try and improve this by setting
# NODE_OPTIONS="--max-old-space-size=XXXXXX", but these dictionaries are so big
# that doesn't even seem to help; they'll still exhaust all the memory. These
# were all tested with --max-old-space-size=25000
INFEASIBLE = ("hu_HU", "ko_KR", "mn_MN")


def write_metadata(dic_path: Path, metadata_dest: Path) -> None:
    """
    As will become abundantly clear, there's a fair amount of guesswork &
    fudging involved to get anything usable here.
    """
    doc = dict(name=f"libreoffice_{dic_path.stem}".lower().replace("-", "_"))

    if dic_path.stem.endswith("_frami"):
        dic_name = dic_path.stem[: -len("_frami")]
    elif dic_path.stem.endswith("-official"):
        dic_name = dic_path.stem[: -len("-official")]
    else:
        dic_name = dic_path.stem

    if (re_match := re.fullmatch(r"([a-z]{2})[-_]([A-Z]{2})", dic_name)) is not None:
        try:
            language = Language.fromalpha2(re_match.group(1).lower())
        except Exception as e:
            raise ValueError(f"{dic_path}: not a language") from e
    elif (
        re_match := re.fullmatch(r"([a-z]{2,3})[-_]([A-Z][a-z]{3})", dic_name)
    ) is not None:
        language_str = re_match.group(1)
        if len(language_str) == 2:
            language = Language.fromalpha2(language_str)
        elif len(language_str) == 3:
            language = Language(language_str)
        else:
            raise ValueError(f"{dic_path}: don't know how to decide language")
        doc["script"] = Script(re_match.group(2)).code
    elif len(dic_name) == 2:
        language = Language.fromalpha2(dic_name)
    elif len(dic_name) == 3:
        language = Language(dic_name)
    elif dic_name == "ca-valencia":
        # no ISO 639-1 or ISO 639-2 code
        # babelfish rejects its IETF name, not recognising the script
        language = None
    else:
        raise ValueError(f"{dic_path}: don't know how to decide language")

    if language is not None:
        try:
            doc["language"] = language.alpha2
        except LanguageConvertError:
            print(
                f"language {language.alpha3} does not have a 2 character language code, omitting"
            )
    metadata_contents = (
        "# @generated by scripts/import_libreoffice.py\n" + tomli_w.dumps(doc)
    )
    metadata_dest.write_text(metadata_contents, encoding="utf-8")
    print(f"wrote metadata to {metadata_dest.relative_to(DATA)}")


# Minimisation with Aho Corasick ported from
# https://github.com/googlefonts/diffenator2/blob/69a873d79811e957aa5824e04d4859717f206c47/src/diffenator2/wordlistbuilder.py#L65-L80
def remove_substring_words(all_words: Iterable[str]) -> set[str]:
    minimised_words = set()
    auto = ahocorasick.Automaton()
    for word in sorted(all_words, key=lambda w: -len(w)):
        auto.add_word(word, word)
        minimised_words.add(word)
    auto.make_automaton()

    for word in all_words:
        for _, found in auto.iter(word):
            if word != found:
                try:
                    minimised_words.remove(found)
                except KeyError:
                    pass
    return minimised_words


# https://github.com/googlefonts/diffenator2/blob/69a873d79811e957aa5824e04d4859717f206c47/scripts/ngram_slim.py#L11-L19
# with functions inlined
def ngram_slim(all_words: Collection[str]) -> set[str]:
    # https://github.com/googlefonts/diffenator2/blob/69a873d79811e957aa5824e04d4859717f206c47/src/diffenator2/wordlistbuilder.py#L9-L13
    def all_ngrams(word: str, size: int) -> Iterator[str]:
        yield from (word[i : i + size] for i in range(max(1, len(word) - size)))

    # https://github.com/googlefonts/diffenator2/blob/69a873d79811e957aa5824e04d4859717f206c47/src/diffenator2/wordlistbuilder.py#L16
    def maybe_add_word(
        bank: set[str],
        word: str,
        ngram_set: set[str],
        keep_chars: set[str] | None = None,
        size: int = 4,
    ):
        if word in bank:
            return False

        if keep_chars is not None and not all(c in keep_chars for c in word):
            return False

        if all(ngram in ngram_set for ngram in all_ngrams(word, size=size)):
            return False

        bank.add(word)

        for ngram in all_ngrams(word, size=size):
            ngram_set.add(ngram)
        return True

    while True:
        bank = set()
        ngram_auto = set()
        for word in sorted(all_words, key=lambda x: -len(x)):
            maybe_add_word(bank, word, ngram_auto)
        if len(bank) == len(all_words):
            break
        all_words = bank

    assert isinstance(all_words, set)
    return all_words


def main(args: Namespace) -> None:
    OUTPUT.mkdir(exist_ok=True)
    for dic_path in sorted(args.libreoffice_repo_path.glob("**/*.dic")):
        print()
        if not dic_path.with_suffix(".aff").exists():
            print(f"skipped {dic_path}: no corresponding .aff file")
            continue
        elif dic_path.stem in INFEASIBLE:
            print(f"skipped {dic_path}: known to be too large")
            continue

        output_path = (OUTPUT / dic_path.stem).with_suffix(".txt")

        if output_path.exists() and args.skip_existing:
            print(f"skipped {dic_path}: already processed")
            continue

        start = time()

        output = subprocess.check_output(
            ("hunspell-reader", "words", "--progress", str(dic_path)),
            text=True,
            encoding="utf-8",
            shell=True,
        )
        all_words = set(output.splitlines())
        try:
            all_words.remove("")
        except KeyError:
            pass
        print(f"extracted {len(all_words)} words")

        print("minimising round #1...")
        r1_words = remove_substring_words(all_words)
        print(
            f"reduced to {len(r1_words)} words,",
            f"{100 - (len(r1_words) / len(all_words) * 100):.1f}% reduction",
        )

        print("minimising round #2...")
        # do ngram slimming second since it's slower but more thorough
        r2_words = ngram_slim(r1_words)
        print(
            f"reduced to {len(r2_words)} words,",
            f"{100 - (len(r2_words) / len(r1_words) * 100):.1f}% reduction",
        )

        output_path.write_text("\n".join(sorted(r2_words)) + "\n", encoding="utf-8")
        print(f"wrote {output_path.relative_to(DATA)} (took {time() - start:.1f}s)")
        write_metadata(dic_path, output_path.with_suffix(".toml"))


if __name__ == "__main__":
    parser = ArgumentParser()
    parser.add_argument(
        "libreoffice_repo_path",
        type=Path,
    )
    parser.add_argument(
        "--skip-existing",
        action="store_true",
    )

    args = parser.parse_args()

    main(args)
